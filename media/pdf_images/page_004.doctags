 <doctag><text><loc_44><loc_47><loc_245><loc_80>results by 0.142 ± 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches - they require training on data that cover intended usage.</text>
<section_header_level_1><loc_44><loc_88><loc_112><loc_94>3.3. Ablation Studies</section_header_level_1>
<section_header_level_1><loc_44><loc_99><loc_183><loc_104>3.3.1. Predicted Features versus Ground Truth</section_header_level_1>
<text><loc_44><loc_109><loc_245><loc_135>While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2.</text>
<otsl><loc_68><loc_142><loc_222><loc_175><ched>Training<ched>Synthesis<lcel><lcel><nl><ecel><ched>Predicted<ched>Ground truth<lcel><nl><rhed>Predicted<fcel>4.526 ± 0.066<fcel>4.449 ± 0.060<lcel><nl><rhed>Ground truth<fcel>4.362 ± 0.066<fcel>4.522 ± 0.055<lcel><nl></otsl>
<text><loc_44><loc_181><loc_245><loc_201>Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms.</text>
<text><loc_44><loc_209><loc_245><loc_268>As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth - a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features.</text>
<section_header_level_1><loc_44><loc_276><loc_127><loc_282>3.3.2. Linear Spectrograms</section_header_level_1>
<text><loc_44><loc_287><loc_245><loc_306>Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim.</text>
<otsl><loc_68><loc_314><loc_222><loc_346><ched>System<ched>MOS<nl><fcel>Tacotron 2 (Linear + G-L)<fcel>3.944 ± 0.091<nl><fcel>Tacotron 2 (Linear + WaveNet)<fcel>4.510 ± 0.054<nl><fcel>Tacotron 2 (Mel + WaveNet)<fcel>4.526 ± 0.066<nl></otsl>
<otsl><loc_265><loc_267><loc_456><loc_310><fcel>Total  layers<fcel>Num  cycles<fcel>Dilation  cycle size (samples / ms)<fcel>Receptive field  (samples / ms)<fcel>MOS<nl><fcel>30<fcel>3<fcel>10<fcel>6,139 / 255.8<fcel>4.526 ± 0.066<nl><fcel>24<fcel>4<fcel>6<fcel>505 / 21.0<fcel>4.547 ± 0.056<nl><fcel>12<fcel>2<fcel>6<fcel>253 / 10.5<fcel>4.481 ± 0.059<nl><fcel>30<fcel>30<fcel>1<fcel>61 / 2.5<fcel>3.930 ± 0.076<nl></otsl>
<otsl><loc_265><loc_314><loc_456><loc_318><ched>Table 4. WaveNet with various layer and receptive field sizes.<nl></otsl>
<section_header_level_1><loc_325><loc_338><loc_387><loc_344>4. CONCLUSION</section_header_level_1>
<text><loc_257><loc_353><loc_456><loc_398>This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.</text>
<section_header_level_1><loc_311><loc_411><loc_403><loc_417>5. ACKNOWLEDGMENTS</section_header_level_1>
<text><loc_257><loc_424><loc_456><loc_456>The authors thank Jan Chorowski, Samy Bengio, Aïron van den Oord, and the WaveNet and Machine Hearing teams for their helpful discussions and advice, as well as Heiga Zen and the Google TTS team for their feedback and assistance with running evaluations. The authors are also grateful to the very thorough reviewers.</text>
</doctag>