by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.

## 3. EXPERIMENTS &amp; RESULTS

## 3.1. Training Setup

Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network.

To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing ) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with β$\_{1}$ = 0 . 9 , β$\_{2}$ = 0 . 999 , c = 10 - 6 and a learning rate of 10 - 3 exponentially decaying to 10 - 5 after starting 50,000 iterations. We also apply L$\_{2}$ regularization with weight 10 - 6 .

We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples.

We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with β$\_{1}$ = 0 . 9 , β$\_{2}$ = 0 . 999 , c = 10 - 8 and a fixed learning rate of 10 - 4 . It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0 . 999 - this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of the logistics layer closer to the eventual distributions.

We train all models on an internal US English dataset [12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g. „16" is written as " sixteen ", i.e., our models are all trained on normalized text.

## 3.2. Evaluation

When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training.

We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon's Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them.

Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful.

Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic

features [8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as conacentative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outperforms all other TTS systems, and results in a MOS comparable to that of the ground truth audio. 1

Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems.

| System                  | MOS           |
|-------------------------|---------------|
| Parametric              | 3.492 ± 0.096 |
| Tacotron (Griffin-Lim)  | 4.001 ± 0.087 |
| Concatenative           | 4.166 ± 0.091 |
| WaveNet (Linguistic)    | 4.341 ± 0.051 |
| Ground truth            | 4.582 ± 0.053 |
| Tacotron 2 (this paper) | 4.526 ± 0.066 |
|                         |               |

We also conduct a side-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of - 0 . 270 ± 0 . 155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 [for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference.

We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling.

Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.148 ± 0 . 124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 ± 0 . 128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie - a statistically insignificant preference towards our

$^{1}$Samples available at https://google.github.io/tacotron/publications/tacotron2.

![page_003_picture_01.png](my_streamlit_app/media/crops/page_003_picture_01.png)

**Image page_003_picture_01.png:** Here's a detailed description of the image you sent:

**Type:** The image is a bar graph, a type of data visualization.

**Purpose:** The graph illustrates responses to a question, likely related to how someone perceives a change or situation.

**Axes:** 
*   **Y-axis (Vertical):**  The vertical axis represents a numerical scale, ranging from 0 to 368.  This indicates the number of respondents.
*   **X-axis (Horizontal):**  The horizontal axis displays the following categories:
    *   “Much Worse”
    *   “Worse”
    *   “Slightly Worse”
    *   “About the Same”
    *   “Slightly Better”
    *   “Better”
    *   “Much Better”

**Data Representation:**
*   **“Much Worse”**: There are 46 responses in this category. The bar for this category is relatively short and positioned at the lowest point on the graph.
*   **“Worse”**: 91 respondents selected this option.
*   **“Slightly Worse”**:  This is the largest category, with 127 responses. The bar for this category is tall, showing the most frequent response.
*   **“About the Same”**: There are 368 respondents who indicated this. This is the tallest bar on the graph, showing the most frequent response.
*   **“Slightly Better”**: 112 responses indicated this. 
*   **“Better”**:  49 respondents chose this.
*   **“Much Better”**:  Only 7 respondents selected this option.  The bar is very short.

**Overall Impression:** The data shows a strong preference for "About the Same" (368 responses), with a relatively high number of people considering the situation "Slightly Worse" (127). A small number of people found the situation "Much Worse" (46) or "Much Better" (7).

If you'd like, you can ask me to analyze this data further, such as calculating percentages or identifying trends.

